# Comprehensive Testing Framework for VPA-NAP Integration

---
# Test Namespace
apiVersion: v1
kind: Namespace
metadata:
  name: vpa-nap-testing
  labels:
    testing: "true"
    tenant-tier: "test"

---
# Test Service Account
apiVersion: v1
kind: ServiceAccount
metadata:
  name: vpa-nap-test-runner
  namespace: vpa-nap-testing

---
# Test Runner ClusterRole
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  name: vpa-nap-test-runner
rules:
- apiGroups: ["autoscaling.k8s.io"]
  resources: ["verticalpodautoscalers", "verticalpodautoscalercheckpoints"]
  verbs: ["*"]
- apiGroups: ["platform.io"]
  resources: ["vpanapcoordinations"]
  verbs: ["*"]
- apiGroups: [""]
  resources: ["pods", "nodes", "events", "configmaps"]
  verbs: ["*"]
- apiGroups: ["apps"]
  resources: ["deployments", "replicasets"]
  verbs: ["*"]
- apiGroups: ["metrics.k8s.io"]
  resources: ["nodes", "pods"]
  verbs: ["get", "list"]
- apiGroups: ["batch"]
  resources: ["jobs"]
  verbs: ["*"]

---
# Test Runner ClusterRoleBinding
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  name: vpa-nap-test-runner
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: vpa-nap-test-runner
subjects:
- kind: ServiceAccount
  name: vpa-nap-test-runner
  namespace: vpa-nap-testing

---
# Test ConfigMap with Test Scripts
apiVersion: v1
kind: ConfigMap
metadata:
  name: vpa-nap-test-scripts
  namespace: vpa-nap-testing
data:
  unit-tests.sh: |
    #!/bin/bash
    # Unit Tests for VPA-NAP Integration
    
    set -euo pipefail
    
    TEST_NAMESPACE="vpa-nap-testing"
    PASSED=0
    FAILED=0
    
    log() {
        echo "$(date '+%Y-%m-%d %H:%M:%S') [UNIT-TEST] $*"
    }
    
    assert_equals() {
        local expected="$1"
        local actual="$2"
        local test_name="$3"
        
        if [[ "$expected" == "$actual" ]]; then
            log "✅ PASS: $test_name"
            ((PASSED++))
        else
            log "❌ FAIL: $test_name - Expected: '$expected', Got: '$actual'"
            ((FAILED++))
        fi
    }
    
    assert_true() {
        local condition="$1"
        local test_name="$2"
        
        if [[ $condition -eq 0 ]]; then
            log "✅ PASS: $test_name"
            ((PASSED++))
        else
            log "❌ FAIL: $test_name"
            ((FAILED++))
        fi
    }
    
    # Test 1: Kyverno Policies Validation
    test_kyverno_policies() {
        log "Testing Kyverno policy syntax..."
        
        # Create a test VPA that should be blocked
        cat <<EOF | kubectl apply --dry-run=client -o yaml - >/dev/null 2>&1
    apiVersion: autoscaling.k8s.io/v1
    kind: VerticalPodAutoscaler
    metadata:
      name: test-invalid-vpa
      namespace: $TEST_NAMESPACE
    spec:
      targetRef:
        apiVersion: apps/v1
        kind: Deployment
        name: test-app
      updatePolicy:
        updateMode: Auto
      resourcePolicy:
        containerPolicies:
        - maxAllowed:
            cpu: "20"  # Should exceed limits
            memory: "50Gi"
    EOF
        
        assert_true $? "Invalid VPA configuration rejected"
    }
    
    # Test 2: Coordinator Resource Creation
    test_coordinator_resources() {
        log "Testing coordinator resource creation..."
        
        # Check if coordinator deployment exists
        kubectl get deployment vpa-nap-coordinator-ha -n platform >/dev/null 2>&1
        assert_true $? "Coordinator deployment exists"
        
        # Check if service exists
        kubectl get service vpa-nap-coordinator -n platform >/dev/null 2>&1
        assert_true $? "Coordinator service exists"
        
        # Check if CRD exists
        kubectl get crd vpanapcoordinations.platform.io >/dev/null 2>&1
        assert_true $? "VPANAPCoordination CRD exists"
    }
    
    # Test 3: VPA Creation and Validation
    test_vpa_creation() {
        log "Testing VPA creation and validation..."
        
        # Create test deployment
        kubectl apply -f - <<EOF
    apiVersion: apps/v1
    kind: Deployment
    metadata:
      name: test-app
      namespace: $TEST_NAMESPACE
      labels:
        vpa-enabled: "true"
        workload-type: "webapp"
    spec:
      replicas: 1
      selector:
        matchLabels:
          app: test-app
      template:
        metadata:
          labels:
            app: test-app
        spec:
          containers:
          - name: test-container
            image: nginx:latest
            resources:
              requests:
                cpu: 100m
                memory: 128Mi
              limits:
                cpu: 500m
                memory: 256Mi
    EOF
        
        # Create valid VPA
        kubectl apply -f - <<EOF
    apiVersion: autoscaling.k8s.io/v1
    kind: VerticalPodAutoscaler
    metadata:
      name: test-app-vpa
      namespace: $TEST_NAMESPACE
      labels:
        nap-coordination: "coordinated"
        tenant-tier: "test"
    spec:
      targetRef:
        apiVersion: apps/v1
        kind: Deployment
        name: test-app
      updatePolicy:
        updateMode: Initial
      resourcePolicy:
        containerPolicies:
        - containerName: test-container
          minAllowed:
            cpu: 50m
            memory: 64Mi
          maxAllowed:
            cpu: 2
            memory: 4Gi
    EOF
        
        # Wait for VPA to be created
        sleep 5
        kubectl get vpa test-app-vpa -n $TEST_NAMESPACE >/dev/null 2>&1
        assert_true $? "VPA created successfully"
        
        # Check if coordination resource was auto-created
        sleep 10
        kubectl get vpanapcoordination test-app-vpa -n $TEST_NAMESPACE >/dev/null 2>&1 || true
        # Note: This might fail if coordinator is not running, which is expected in testing
    }
    
    # Test 4: Circuit Breaker Logic
    test_circuit_breaker() {
        log "Testing circuit breaker logic..."
        
        # Simulate high eviction scenario (create events)
        for i in {1..25}; do
            kubectl create event \
                --namespace $TEST_NAMESPACE \
                --type Warning \
                --reason Evicted \
                --message "Test eviction event $i" \
                --source TestFramework \
                --related-api-version v1 \
                --related-kind Pod \
                --related-name test-pod-$i 2>/dev/null || true
        done
        
        # Check if circuit breaker would trigger (simulate)
        local eviction_count=$(kubectl get events -n $TEST_NAMESPACE \
            --field-selector reason=Evicted \
            --output json | jq '.items | length')
        
        if [[ $eviction_count -gt 20 ]]; then
            log "✅ PASS: Circuit breaker would trigger (eviction count: $eviction_count)"
            ((PASSED++))
        else
            log "❌ FAIL: Circuit breaker threshold not met (eviction count: $eviction_count)"
            ((FAILED++))
        fi
    }
    
    # Test 5: Tenant Tier Validation
    test_tenant_tiers() {
        log "Testing tenant tier configurations..."
        
        # Test dev tier VPA
        kubectl apply -f - <<EOF
    apiVersion: autoscaling.k8s.io/v1
    kind: VerticalPodAutoscaler
    metadata:
      name: dev-tier-vpa
      namespace: $TEST_NAMESPACE
      labels:
        nap-coordination: "coordinated"
    spec:
      targetRef:
        apiVersion: apps/v1
        kind: Deployment
        name: test-app
      updatePolicy:
        updateMode: Auto  # Should be allowed for dev
      resourcePolicy:
        containerPolicies:
        - maxAllowed:
            cpu: "1"  # Within dev limits
            memory: "2Gi"
    EOF
        
        kubectl get vpa dev-tier-vpa -n $TEST_NAMESPACE >/dev/null 2>&1
        assert_true $? "Dev tier VPA created"
    }
    
    # Run all tests
    run_unit_tests() {
        log "Starting VPA-NAP unit tests..."
        
        test_kyverno_policies
        test_coordinator_resources
        test_vpa_creation
        test_circuit_breaker
        test_tenant_tiers
        
        log "Unit tests completed: $PASSED passed, $FAILED failed"
        
        if [[ $FAILED -gt 0 ]]; then
            exit 1
        fi
    }
    
    # Cleanup function
    cleanup() {
        log "Cleaning up test resources..."
        kubectl delete deployment test-app -n $TEST_NAMESPACE --ignore-not-found=true
        kubectl delete vpa --all -n $TEST_NAMESPACE --ignore-not-found=true
        kubectl delete vpanapcoordination --all -n $TEST_NAMESPACE --ignore-not-found=true
        kubectl delete events --all -n $TEST_NAMESPACE --ignore-not-found=true
    }
    
    trap cleanup EXIT
    run_unit_tests

  integration-tests.sh: |
    #!/bin/bash
    # Integration Tests for VPA-NAP System
    
    set -euo pipefail
    
    TEST_NAMESPACE="vpa-nap-testing"
    PASSED=0
    FAILED=0
    
    log() {
        echo "$(date '+%Y-%m-%d %H:%M:%S') [INTEGRATION-TEST] $*"
    }
    
    # Test VPA-NAP Coordination Flow
    test_coordination_flow() {
        log "Testing VPA-NAP coordination flow..."
        
        # Create a realistic workload
        kubectl apply -f - <<EOF
    apiVersion: apps/v1
    kind: Deployment
    metadata:
      name: load-test-app
      namespace: $TEST_NAMESPACE
      labels:
        vpa-enabled: "true"
        workload-type: "webapp"
    spec:
      replicas: 2
      selector:
        matchLabels:
          app: load-test-app
      template:
        metadata:
          labels:
            app: load-test-app
        spec:
          containers:
          - name: app
            image: nginx:latest
            resources:
              requests:
                cpu: 100m
                memory: 128Mi
            ports:
            - containerPort: 80
    EOF
        
        # Create VPA for the workload
        kubectl apply -f - <<EOF
    apiVersion: autoscaling.k8s.io/v1
    kind: VerticalPodAutoscaler
    metadata:
      name: load-test-vpa
      namespace: $TEST_NAMESPACE
      labels:
        nap-coordination: "coordinated"
    spec:
      targetRef:
        apiVersion: apps/v1
        kind: Deployment
        name: load-test-app
      updatePolicy:
        updateMode: Initial
      resourcePolicy:
        containerPolicies:
        - containerName: app
          minAllowed:
            cpu: 50m
            memory: 64Mi
          maxAllowed:
            cpu: 4
            memory: 8Gi
    EOF
        
        # Wait for VPA to generate recommendations
        log "Waiting for VPA recommendations..."
        for i in {1..30}; do
            if kubectl get vpa load-test-vpa -n $TEST_NAMESPACE -o jsonpath='{.status.recommendation}' | grep -q cpu; then
                log "✅ PASS: VPA generated recommendations"
                ((PASSED++))
                break
            fi
            sleep 10
        done
        
        if [[ $i -eq 30 ]]; then
            log "❌ FAIL: VPA did not generate recommendations in 5 minutes"
            ((FAILED++))
        fi
    }
    
    # Test Load Generation and Resource Scaling
    test_load_scaling() {
        log "Testing load generation and scaling behavior..."
        
        # Create load generator
        kubectl apply -f - <<EOF
    apiVersion: batch/v1
    kind: Job
    metadata:
      name: load-generator
      namespace: $TEST_NAMESPACE
    spec:
      template:
        spec:
          containers:
          - name: load-gen
            image: busybox
            command: ["/bin/sh"]
            args:
            - -c
            - |
              # Generate CPU load
              while true; do
                dd if=/dev/zero of=/dev/null bs=1M count=100 2>/dev/null || true
                sleep 1
              done
            resources:
              requests:
                cpu: 10m
                memory: 32Mi
          restartPolicy: Never
      backoffLimit: 0
    EOF
        
        # Monitor resource usage
        sleep 30
        local cpu_usage=$(kubectl top pod -n $TEST_NAMESPACE --no-headers | awk '{print $2}' | head -1)
        if [[ -n "$cpu_usage" ]]; then
            log "✅ PASS: Load generator created CPU usage: $cpu_usage"
            ((PASSED++))
        else
            log "❌ FAIL: Could not measure CPU usage"
            ((FAILED++))
        fi
    }
    
    # Test Multi-Tenant Isolation
    test_tenant_isolation() {
        log "Testing multi-tenant isolation..."
        
        # Create tenant namespaces
        for tier in dev std premium; do
            kubectl create namespace tenant-${tier}-test --dry-run=client -o yaml | kubectl apply -f -
            
            # Create VPA in each tenant namespace
            kubectl apply -f - <<EOF
    apiVersion: apps/v1
    kind: Deployment
    metadata:
      name: tenant-app
      namespace: tenant-${tier}-test
      labels:
        vpa-enabled: "true"
    spec:
      replicas: 1
      selector:
        matchLabels:
          app: tenant-app
      template:
        metadata:
          labels:
            app: tenant-app
        spec:
          containers:
          - name: app
            image: nginx:latest
            resources:
              requests:
                cpu: 50m
                memory: 64Mi
    ---
    apiVersion: autoscaling.k8s.io/v1
    kind: VerticalPodAutoscaler
    metadata:
      name: tenant-vpa
      namespace: tenant-${tier}-test
      labels:
        nap-coordination: "coordinated"
        tenant-tier: "$tier"
    spec:
      targetRef:
        apiVersion: apps/v1
        kind: Deployment
        name: tenant-app
      updatePolicy:
        updateMode: Initial
    EOF
        done
        
        # Verify VPAs were created with proper tier settings
        for tier in dev std premium; do
            if kubectl get vpa tenant-vpa -n tenant-${tier}-test >/dev/null 2>&1; then
                log "✅ PASS: VPA created for $tier tier"
                ((PASSED++))
            else
                log "❌ FAIL: VPA not created for $tier tier"
                ((FAILED++))
            fi
        done
    }
    
    # Test Failure Recovery
    test_failure_recovery() {
        log "Testing failure recovery scenarios..."
        
        # Simulate coordinator failure
        kubectl scale deployment vpa-nap-coordinator-ha -n platform --replicas=0 2>/dev/null || true
        sleep 10
        
        # Verify VPAs still function (should be in safe mode)
        kubectl get vpa --all-namespaces >/dev/null 2>&1
        if [[ $? -eq 0 ]]; then
            log "✅ PASS: VPAs remain accessible during coordinator failure"
            ((PASSED++))
        else
            log "❌ FAIL: VPAs not accessible during coordinator failure"
            ((FAILED++))
        fi
        
        # Restore coordinator
        kubectl scale deployment vpa-nap-coordinator-ha -n platform --replicas=3 2>/dev/null || true
        sleep 30
    }
    
    # Run all integration tests
    run_integration_tests() {
        log "Starting VPA-NAP integration tests..."
        
        test_coordination_flow
        test_load_scaling
        test_tenant_isolation
        test_failure_recovery
        
        log "Integration tests completed: $PASSED passed, $FAILED failed"
        
        if [[ $FAILED -gt 0 ]]; then
            exit 1
        fi
    }
    
    # Cleanup function
    cleanup_integration() {
        log "Cleaning up integration test resources..."
        kubectl delete job load-generator -n $TEST_NAMESPACE --ignore-not-found=true
        kubectl delete deployment load-test-app -n $TEST_NAMESPACE --ignore-not-found=true
        kubectl delete vpa load-test-vpa -n $TEST_NAMESPACE --ignore-not-found=true
        
        for tier in dev std premium; do
            kubectl delete namespace tenant-${tier}-test --ignore-not-found=true
        done
    }
    
    trap cleanup_integration EXIT
    run_integration_tests

  performance-tests.sh: |
    #!/bin/bash
    # Performance Tests for VPA-NAP Integration
    
    set -euo pipefail
    
    TEST_NAMESPACE="vpa-nap-testing"
    
    log() {
        echo "$(date '+%Y-%m-%d %H:%M:%S') [PERF-TEST] $*"
    }
    
    # Test VPA Response Time
    test_vpa_response_time() {
        log "Testing VPA response time under load..."
        
        # Create multiple VPAs simultaneously
        for i in {1..10}; do
            kubectl apply -f - <<EOF
    apiVersion: apps/v1
    kind: Deployment
    metadata:
      name: perf-test-app-$i
      namespace: $TEST_NAMESPACE
    spec:
      replicas: 1
      selector:
        matchLabels:
          app: perf-test-app-$i
      template:
        metadata:
          labels:
            app: perf-test-app-$i
        spec:
          containers:
          - name: app
            image: nginx:latest
            resources:
              requests:
                cpu: 10m
                memory: 32Mi
    ---
    apiVersion: autoscaling.k8s.io/v1
    kind: VerticalPodAutoscaler
    metadata:
      name: perf-test-vpa-$i
      namespace: $TEST_NAMESPACE
      labels:
        nap-coordination: "coordinated"
    spec:
      targetRef:
        apiVersion: apps/v1
        kind: Deployment
        name: perf-test-app-$i
      updatePolicy:
        updateMode: Initial
    EOF
        done &
        
        local start_time=$(date +%s)
        wait
        local create_time=$(($(date +%s) - start_time))
        
        log "VPA creation time for 10 VPAs: ${create_time}s"
        
        # Measure recommendation generation time
        local recommendations_ready=0
        start_time=$(date +%s)
        
        while [[ $recommendations_ready -lt 10 ]] && [[ $(($(date +%s) - start_time)) -lt 300 ]]; do
            recommendations_ready=0
            for i in {1..10}; do
                if kubectl get vpa perf-test-vpa-$i -n $TEST_NAMESPACE -o jsonpath='{.status.recommendation}' 2>/dev/null | grep -q cpu; then
                    ((recommendations_ready++))
                fi
            done
            sleep 5
        done
        
        local recommendation_time=$(($(date +%s) - start_time))
        log "VPA recommendation generation time: ${recommendation_time}s"
        
        if [[ $recommendation_time -lt 180 ]]; then
            log "✅ PASS: VPA recommendations generated within 3 minutes"
        else
            log "❌ FAIL: VPA recommendations took longer than 3 minutes"
        fi
    }
    
    # Test Memory Usage
    test_memory_usage() {
        log "Testing coordinator memory usage..."
        
        # Get coordinator memory usage
        local memory_usage=$(kubectl top pod -n platform -l app=vpa-nap-coordinator --no-headers 2>/dev/null | awk '{print $3}' | head -1)
        
        if [[ -n "$memory_usage" ]]; then
            log "Coordinator memory usage: $memory_usage"
            
            # Parse memory value (remove 'Mi' suffix and convert to number)
            local memory_mb=$(echo "$memory_usage" | sed 's/Mi//')
            if [[ $memory_mb -lt 500 ]]; then
                log "✅ PASS: Memory usage within acceptable limits"
            else
                log "❌ FAIL: Memory usage too high: ${memory_mb}Mi"
            fi
        else
            log "⚠️  WARN: Could not measure coordinator memory usage"
        fi
    }
    
    # Test Concurrent Operations
    test_concurrent_operations() {
        log "Testing concurrent VPA operations..."
        
        # Create, update, and delete VPAs concurrently
        for i in {1..5}; do
            (
                kubectl apply -f - <<EOF
    apiVersion: autoscaling.k8s.io/v1
    kind: VerticalPodAutoscaler
    metadata:
      name: concurrent-vpa-$i
      namespace: $TEST_NAMESPACE
    spec:
      targetRef:
        apiVersion: apps/v1
        kind: Deployment
        name: perf-test-app-1
      updatePolicy:
        updateMode: Initial
    EOF
                sleep 2
                kubectl patch vpa concurrent-vpa-$i -n $TEST_NAMESPACE --patch '{"spec":{"updatePolicy":{"updateMode":"Off"}}}'
                sleep 2
                kubectl delete vpa concurrent-vpa-$i -n $TEST_NAMESPACE
            ) &
        done
        
        wait
        log "✅ PASS: Concurrent operations completed"
    }
    
    # Run performance tests
    run_performance_tests() {
        log "Starting VPA-NAP performance tests..."
        
        test_vpa_response_time
        test_memory_usage
        test_concurrent_operations
        
        log "Performance tests completed"
    }
    
    # Cleanup
    cleanup_performance() {
        log "Cleaning up performance test resources..."
        kubectl delete deployment --all -n $TEST_NAMESPACE --ignore-not-found=true
        kubectl delete vpa --all -n $TEST_NAMESPACE --ignore-not-found=true
    }
    
    trap cleanup_performance EXIT
    run_performance_tests

  chaos-tests.sh: |
    #!/bin/bash
    # Chaos Engineering Tests for VPA-NAP Integration
    
    set -euo pipefail
    
    log() {
        echo "$(date '+%Y-%m-%d %H:%M:%S') [CHAOS-TEST] $*"
    }
    
    # Test Network Partition
    test_network_partition() {
        log "Testing network partition scenarios..."
        
        # This would require chaos engineering tools like Chaos Mesh
        # For now, we'll simulate by scaling coordinator to 0 and back
        
        kubectl scale deployment vpa-nap-coordinator-ha -n platform --replicas=0 2>/dev/null || true
        log "Simulated network partition (coordinator scaled to 0)"
        
        sleep 60
        
        # Check that VPAs still exist and are accessible
        kubectl get vpa --all-namespaces >/dev/null 2>&1
        if [[ $? -eq 0 ]]; then
            log "✅ PASS: VPAs remain accessible during partition"
        else
            log "❌ FAIL: VPAs not accessible during partition"
        fi
        
        # Restore
        kubectl scale deployment vpa-nap-coordinator-ha -n platform --replicas=3 2>/dev/null || true
        sleep 30
    }
    
    # Test Resource Exhaustion
    test_resource_exhaustion() {
        log "Testing resource exhaustion scenarios..."
        
        # Create resource-intensive pods
        kubectl apply -f - <<EOF
    apiVersion: apps/v1
    kind: Deployment
    metadata:
      name: resource-hog
      namespace: vpa-nap-testing
    spec:
      replicas: 1
      selector:
        matchLabels:
          app: resource-hog
      template:
        metadata:
          labels:
            app: resource-hog
        spec:
          containers:
          - name: cpu-hog
            image: busybox
            command: ["/bin/sh"]
            args:
            - -c
            - "while true; do dd if=/dev/zero of=/dev/null; done"
            resources:
              requests:
                cpu: 2
                memory: 1Gi
              limits:
                cpu: 4
                memory: 2Gi
    EOF
        
        sleep 30
        
        # Verify coordinator still responds
        kubectl get pods -n platform -l app=vpa-nap-coordinator >/dev/null 2>&1
        if [[ $? -eq 0 ]]; then
            log "✅ PASS: Coordinator remains responsive under resource pressure"
        else
            log "❌ FAIL: Coordinator not responsive under resource pressure"
        fi
        
        kubectl delete deployment resource-hog -n vpa-nap-testing --ignore-not-found=true
    }
    
    # Run chaos tests
    run_chaos_tests() {
        log "Starting VPA-NAP chaos tests..."
        
        test_network_partition
        test_resource_exhaustion
        
        log "Chaos tests completed"
    }
    
    run_chaos_tests

---
# Test Execution Jobs
apiVersion: batch/v1
kind: Job
metadata:
  name: vpa-nap-unit-tests
  namespace: vpa-nap-testing
spec:
  template:
    spec:
      serviceAccountName: vpa-nap-test-runner
      containers:
      - name: unit-tests
        image: bitnami/kubectl:latest
        command: ["/bin/bash"]
        args: ["/scripts/unit-tests.sh"]
        volumeMounts:
        - name: test-scripts
          mountPath: /scripts
        resources:
          requests:
            cpu: 100m
            memory: 128Mi
          limits:
            cpu: 500m
            memory: 256Mi
      volumes:
      - name: test-scripts
        configMap:
          name: vpa-nap-test-scripts
          defaultMode: 0755
      restartPolicy: Never
  backoffLimit: 3

---
# CronJob for Regular Testing
apiVersion: batch/v1
kind: CronJob
metadata:
  name: vpa-nap-integration-tests
  namespace: vpa-nap-testing
spec:
  schedule: "0 2 * * *"  # Daily at 2 AM
  jobTemplate:
    spec:
      template:
        spec:
          serviceAccountName: vpa-nap-test-runner
          containers:
          - name: integration-tests
            image: bitnami/kubectl:latest
            command: ["/bin/bash"]
            args: ["/scripts/integration-tests.sh"]
            volumeMounts:
            - name: test-scripts
              mountPath: /scripts
            resources:
              requests:
                cpu: 200m
                memory: 256Mi
              limits:
                cpu: 1
                memory: 512Mi
          volumes:
          - name: test-scripts
            configMap:
              name: vpa-nap-test-scripts
              defaultMode: 0755
          restartPolicy: OnFailure

---
# Performance Test CronJob
apiVersion: batch/v1
kind: CronJob
metadata:
  name: vpa-nap-performance-tests
  namespace: vpa-nap-testing
spec:
  schedule: "0 3 * * 0"  # Weekly on Sunday at 3 AM
  jobTemplate:
    spec:
      template:
        spec:
          serviceAccountName: vpa-nap-test-runner
          containers:
          - name: performance-tests
            image: bitnami/kubectl:latest
            command: ["/bin/bash"]
            args: ["/scripts/performance-tests.sh"]
            volumeMounts:
            - name: test-scripts
              mountPath: /scripts
            resources:
              requests:
                cpu: 500m
                memory: 512Mi
              limits:
                cpu: 2
                memory: 1Gi
          volumes:
          - name: test-scripts
            configMap:
              name: vpa-nap-test-scripts
              defaultMode: 0755
          restartPolicy: OnFailure

---
# Test Results ConfigMap
apiVersion: v1
kind: ConfigMap
metadata:
  name: test-results-template
  namespace: vpa-nap-testing
data:
  results.yaml: |
    testSuite: vpa-nap-integration
    timestamp: ""
    results:
      unitTests:
        passed: 0
        failed: 0
        duration: ""
      integrationTests:
        passed: 0
        failed: 0
        duration: ""
      performanceTests:
        metrics:
          vpaCreationTime: ""
          recommendationTime: ""
          memoryUsage: ""
        passed: 0
        failed: 0
      chaosTests:
        scenarios:
          networkPartition: ""
          resourceExhaustion: ""
        passed: 0
        failed: 0