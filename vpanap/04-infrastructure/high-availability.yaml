# High Availability Configuration for VPA-NAP Integration

---
# HA Deployment for VPA-NAP Coordinator
apiVersion: apps/v1
kind: Deployment
metadata:
  name: vpa-nap-coordinator-ha
  namespace: platform
  labels:
    app: vpa-nap-coordinator
    tier: platform
    component: coordination
spec:
  replicas: 3  # Multi-replica for HA
  strategy:
    type: RollingUpdate
    rollingUpdate:
      maxUnavailable: 1
      maxSurge: 1
  selector:
    matchLabels:
      app: vpa-nap-coordinator
  template:
    metadata:
      labels:
        app: vpa-nap-coordinator
      annotations:
        prometheus.io/scrape: "true"
        prometheus.io/port: "8080"
        prometheus.io/path: "/metrics"
    spec:
      serviceAccountName: vpa-nap-coordinator-secure
      # Anti-affinity to spread across nodes
      affinity:
        podAntiAffinity:
          preferredDuringSchedulingIgnoredDuringExecution:
          - weight: 100
            podAffinityTerm:
              labelSelector:
                matchExpressions:
                - key: app
                  operator: In
                  values:
                  - vpa-nap-coordinator
              topologyKey: kubernetes.io/hostname
        nodeAffinity:
          preferredDuringSchedulingIgnoredDuringExecution:
          - weight: 100
            preference:
              matchExpressions:
              - key: node-pool
                operator: In
                values:
                - system
                - platform
      # Tolerate system node taints
      tolerations:
      - key: "CriticalAddonsOnly"
        operator: "Exists"
      - key: "node-role.kubernetes.io/control-plane"
        operator: "Exists"
        effect: "NoSchedule"
      topologySpreadConstraints:
      - maxSkew: 1
        topologyKey: topology.kubernetes.io/zone
        whenUnsatisfiable: DoNotSchedule
        labelSelector:
          matchLabels:
            app: vpa-nap-coordinator
      terminationGracePeriodSeconds: 30
      securityContext:
        runAsNonRoot: true
        runAsUser: 65534
        fsGroup: 65534
      containers:
      - name: coordinator
        image: bitnami/kubectl:latest
        imagePullPolicy: Always
        command: ["/bin/bash"]
        args: ["/scripts/coordinator-ha.sh"]
        env:
        - name: COORDINATOR_ID
          valueFrom:
            fieldRef:
              fieldPath: metadata.name
        - name: NAMESPACE
          valueFrom:
            fieldRef:
              fieldPath: metadata.namespace
        - name: ELECTION_NAMESPACE
          value: "platform"
        - name: ELECTION_ID
          value: "vpa-nap-coordinator"
        - name: CIRCUIT_BREAKER_NODE_CHANGES_THRESHOLD
          value: "5"
        - name: CIRCUIT_BREAKER_EVICTIONS_THRESHOLD
          value: "20"
        - name: COOLDOWN_MINUTES
          value: "30"
        - name: CHECK_INTERVAL
          value: "60"
        - name: LEADER_ELECTION_ENABLED
          value: "true"
        ports:
        - containerPort: 8080
          name: metrics
          protocol: TCP
        volumeMounts:
        - name: scripts
          mountPath: /scripts
          readOnly: true
        - name: tmp
          mountPath: /tmp
        resources:
          requests:
            cpu: 100m
            memory: 128Mi
          limits:
            cpu: 500m
            memory: 256Mi
        livenessProbe:
          httpGet:
            path: /healthz
            port: 8080
          initialDelaySeconds: 30
          periodSeconds: 30
          failureThreshold: 3
        readinessProbe:
          httpGet:
            path: /ready
            port: 8080
          initialDelaySeconds: 5
          periodSeconds: 10
          failureThreshold: 3
        securityContext:
          allowPrivilegeEscalation: false
          runAsNonRoot: true
          runAsUser: 65534
          readOnlyRootFilesystem: true
          capabilities:
            drop:
            - ALL
      volumes:
      - name: scripts
        configMap:
          name: vpa-nap-coordinator-scripts-ha
          defaultMode: 0755
      - name: tmp
        emptyDir: {}

---
# Service for HA Coordinator
apiVersion: v1
kind: Service
metadata:
  name: vpa-nap-coordinator
  namespace: platform
  labels:
    app: vpa-nap-coordinator
  annotations:
    prometheus.io/scrape: "true"
    prometheus.io/port: "8080"
    prometheus.io/path: "/metrics"
spec:
  type: ClusterIP
  ports:
  - port: 8080
    targetPort: 8080
    protocol: TCP
    name: metrics
  selector:
    app: vpa-nap-coordinator

---
# ConfigMap with HA Scripts
apiVersion: v1
kind: ConfigMap
metadata:
  name: vpa-nap-coordinator-scripts-ha
  namespace: platform
data:
  coordinator-ha.sh: |
    #!/bin/bash
    # VPA-NAP Coordinator with High Availability (Leader Election)
    
    set -euo pipefail
    
    # Configuration
    ELECTION_NAMESPACE=${ELECTION_NAMESPACE:-platform}
    ELECTION_ID=${ELECTION_ID:-vpa-nap-coordinator}
    COORDINATOR_ID=${COORDINATOR_ID:-$(hostname)}
    LEADER_ELECTION_ENABLED=${LEADER_ELECTION_ENABLED:-true}
    LEASE_DURATION=${LEASE_DURATION:-15}
    RENEW_DEADLINE=${RENEW_DEADLINE:-10}
    RETRY_PERIOD=${RETRY_PERIOD:-2}
    
    log() {
        echo "$(date '+%Y-%m-%d %H:%M:%S') [$COORDINATOR_ID] $*"
    }
    
    # Health check endpoints
    start_health_server() {
        mkdir -p /tmp
        
        # Start simple HTTP server for health checks
        cat > /tmp/health-server.sh << 'EOF'
    #!/bin/bash
    while true; do
      echo -e "HTTP/1.1 200 OK\r\nContent-Length: 2\r\n\r\nOK" | nc -l -p 8080
    done
    EOF
        chmod +x /tmp/health-server.sh
        /tmp/health-server.sh &
        HEALTH_PID=$!
        
        # Create readiness indicator
        touch /tmp/ready
    }
    
    # Leader election using Kubernetes lease
    acquire_leadership() {
        local current_leader=""
        local lease_exists=false
        
        log "Attempting to acquire leadership..."
        
        # Check if lease exists
        if kubectl get lease "$ELECTION_ID" -n "$ELECTION_NAMESPACE" >/dev/null 2>&1; then
            lease_exists=true
            current_leader=$(kubectl get lease "$ELECTION_ID" -n "$ELECTION_NAMESPACE" \
                -o jsonpath='{.spec.holderIdentity}' 2>/dev/null || echo "")
        fi
        
        if [[ "$current_leader" == "$COORDINATOR_ID" ]]; then
            log "Already the leader"
            return 0
        fi
        
        # Try to acquire or create lease
        if [[ "$lease_exists" == "false" ]]; then
            # Create new lease
            kubectl apply -f - <<EOF
    apiVersion: coordination.k8s.io/v1
    kind: Lease
    metadata:
      name: $ELECTION_ID
      namespace: $ELECTION_NAMESPACE
    spec:
      holderIdentity: $COORDINATOR_ID
      leaseDurationSeconds: $LEASE_DURATION
      renewTime: "$(date -u +"%Y-%m-%dT%H:%M:%SZ")"
      acquireTime: "$(date -u +"%Y-%m-%dT%H:%M:%SZ")"
    EOF
            log "Created new lease and became leader"
            return 0
        else
            # Check if current lease is expired
            local renew_time=$(kubectl get lease "$ELECTION_ID" -n "$ELECTION_NAMESPACE" \
                -o jsonpath='{.spec.renewTime}' 2>/dev/null || echo "")
            local current_time=$(date -u +"%Y-%m-%dT%H:%M:%SZ")
            
            if [[ -n "$renew_time" ]]; then
                local lease_age=$(($(date -d "$current_time" +%s) - $(date -d "$renew_time" +%s)))
                if [[ $lease_age -gt $LEASE_DURATION ]]; then
                    log "Current lease expired, attempting to take over..."
                    kubectl patch lease "$ELECTION_ID" -n "$ELECTION_NAMESPACE" \
                        --type merge --patch "{
                            \"spec\": {
                                \"holderIdentity\": \"$COORDINATOR_ID\",
                                \"renewTime\": \"$current_time\",
                                \"acquireTime\": \"$current_time\"
                            }
                        }"
                    log "Acquired leadership from expired lease"
                    return 0
                fi
            fi
        fi
        
        return 1  # Failed to acquire leadership
    }
    
    # Renew leadership lease
    renew_leadership() {
        kubectl patch lease "$ELECTION_ID" -n "$ELECTION_NAMESPACE" \
            --type merge --patch "{
                \"spec\": {
                    \"renewTime\": \"$(date -u +"%Y-%m-%dT%H:%M:%SZ")\"
                }
            }"
    }
    
    # Check if we are the current leader
    is_leader() {
        if [[ "$LEADER_ELECTION_ENABLED" != "true" ]]; then
            return 0  # Always leader if election disabled
        fi
        
        local current_leader=$(kubectl get lease "$ELECTION_ID" -n "$ELECTION_NAMESPACE" \
            -o jsonpath='{.spec.holderIdentity}' 2>/dev/null || echo "")
        [[ "$current_leader" == "$COORDINATOR_ID" ]]
    }
    
    # Release leadership
    release_leadership() {
        if is_leader; then
            log "Releasing leadership..."
            kubectl delete lease "$ELECTION_ID" -n "$ELECTION_NAMESPACE" 2>/dev/null || true
        fi
    }
    
    # Import coordination functions from main script
    source /scripts/coordinator.sh
    
    # Main HA coordination loop
    main_ha() {
        log "Starting HA VPA-NAP coordinator (ID: $COORDINATOR_ID)"
        start_health_server
        
        while true; do
            if [[ "$LEADER_ELECTION_ENABLED" == "true" ]]; then
                if acquire_leadership; then
                    log "Running as leader"
                    
                    # Run coordination logic
                    main_coordination_cycle
                    
                    # Renew leadership
                    if ! renew_leadership; then
                        log "Failed to renew leadership"
                        sleep $RETRY_PERIOD
                        continue
                    fi
                else
                    log "Not the leader, standby mode"
                    sleep $RETRY_PERIOD
                    continue
                fi
            else
                log "Leader election disabled, running coordination"
                main_coordination_cycle
            fi
            
            sleep "$CHECK_INTERVAL"
        done
    }
    
    # Graceful shutdown
    cleanup() {
        log "Shutting down coordinator..."
        release_leadership
        kill $HEALTH_PID 2>/dev/null || true
        exit 0
    }
    
    trap cleanup TERM INT
    main_ha "$@"

  coordinator-functions.sh: |
    #!/bin/bash
    # Shared coordination functions (extracted from main coordinator)
    
    main_coordination_cycle() {
        # Get all VPAs with coordination enabled
        kubectl get vpa --all-namespaces \
            -o jsonpath='{range .items[*]}{.metadata.namespace} {.metadata.name}{"\n"}{end}' | \
            while read -r namespace vpa_name; do
                if [[ -n "$namespace" ]] && [[ -n "$vpa_name" ]]; then
                    # Check if this VPA has coordination enabled
                    local coordination_mode=$(kubectl get vpa "$vpa_name" -n "$namespace" \
                        -o jsonpath='{.metadata.labels.nap-coordination}' 2>/dev/null || echo "")
                    
                    if [[ "$coordination_mode" == "coordinated" ]]; then
                        monitor_vpa "$namespace" "$vpa_name"
                    fi
                fi
            done
    }

---
# PodDisruptionBudget for HA
apiVersion: policy/v1
kind: PodDisruptionBudget
metadata:
  name: vpa-nap-coordinator-pdb
  namespace: platform
spec:
  minAvailable: 1
  selector:
    matchLabels:
      app: vpa-nap-coordinator

---
# Horizontal Pod Autoscaler for Load-based Scaling
apiVersion: autoscaling/v2
kind: HorizontalPodAutoscaler
metadata:
  name: vpa-nap-coordinator-hpa
  namespace: platform
spec:
  scaleTargetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: vpa-nap-coordinator-ha
  minReplicas: 3
  maxReplicas: 5
  metrics:
  - type: Resource
    resource:
      name: cpu
      target:
        type: Utilization
        averageUtilization: 70
  - type: Resource
    resource:
      name: memory
      target:
        type: Utilization
        averageUtilization: 80
  behavior:
    scaleUp:
      stabilizationWindowSeconds: 300
      policies:
      - type: Pods
        value: 1
        periodSeconds: 300
    scaleDown:
      stabilizationWindowSeconds: 300
      policies:
      - type: Pods
        value: 1
        periodSeconds: 300

---
# ServiceMonitor for HA Metrics
apiVersion: monitoring.coreos.com/v1
kind: ServiceMonitor
metadata:
  name: vpa-nap-coordinator-ha
  namespace: platform
  labels:
    app: vpa-nap-coordinator
spec:
  selector:
    matchLabels:
      app: vpa-nap-coordinator
  endpoints:
  - port: metrics
    interval: 30s
    path: /metrics

---

# VPA State Coordination Only
# Note: Using existing cluster backup framework for cluster-level backup
# VPA-specific coordination and checkpoints are handled by the coordinator
